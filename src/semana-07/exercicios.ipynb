{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9ecfbdb0",
   "metadata": {},
   "source": [
    "## **Roteiro de Prática: Construindo sua RNA com PyTorch**\n",
    "\n",
    "**Documentos importantes para consultas e aprendizado:**\n",
    "\n",
    "[PyTorch documentation — PyTorch 2.7 documentation](https://docs.pytorch.org/docs/stable/index.html)\n",
    "\n",
    "[Zero to Mastery Learn PyTorch for Deep Learning](https://www.learnpytorch.io/)\n",
    "\n",
    "Vamos construir, treinar e avaliar uma rede neural artificial usando PyTorch para resolver um problema clássico: prever quem sobreviveu ao desastre do Titanic.\n",
    "\n",
    "Vamos usar o trabalho dos Profs. Storopoli & Souza disponível no Google Colab:\n",
    "\n",
    "```\n",
    "Storopoli & Souza (2020). Ciência de Dados com Python: pandas, matplotlib, Scikit-Learn, TensorFlow e PyTorch. \n",
    "\n",
    "```\n",
    "\n",
    "https://colab.research.google.com/github/storopoli/ciencia-de-dados/blob/main/notebooks/Aula_18_b_Redes_Neurais_com_PyTorch.ipynb#scrollTo=JjcTgN5kDvqS"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7ff5453",
   "metadata": {},
   "source": [
    "# Exercícios\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f6964fa",
   "metadata": {},
   "source": [
    "\n",
    "# 1 - Com base no material apresentado no notebook, o que é uma função de ativação (como a ReLU)? Por que normalmente usamos entre as camadas?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cee6859d",
   "metadata": {},
   "source": [
    "\n",
    "> De acordo com o material, uma função de ativação tem como objetivo processar a combinação linear entre os inputs e os pesos sinápticos de um neurônio, gerando assim um sinal de saída. Além disso, essas funções são usadas entre as camadas da rede neural para permitir que a rede consiga aprender padrões mais complexos, já que elas introduzem não linearidade no modelo.\n",
    "\n",
    "> Durante o processo de propagação (feedforward), os dados passam pela rede e são transformados pelas funções de ativação. Em seguida, calcula-se a função custo (uma métrica de erro), e por meio da retropropagação (backpropagation), os pesos dos neurônios são ajustados para melhorar os resultados ao longo das épocas (epoch).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1cc6e912",
   "metadata": {},
   "source": [
    "\n",
    "# 2 -  Explique o que cada uma das seguintes linhas de código faz e por que ela é necessária:\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d62ace9e",
   "metadata": {},
   "source": [
    "\n",
    "1. model.train()\n",
    "> Prepara o modelo para o processo de treinamento, ativando comportamentos específicos como Dropout e BatchNorm com estatísticas do batch. Isso é essencial para que o modelo aprenda corretamente durante o treino.\n",
    "2. optimizer.step()\n",
    "> É responsável por atualizar os pesos do modelo, usando uma regra de otimização - neste caso, Adam - com base nos gradientes calculados durante a backpropagation. É necessária pois permite que o modelo melhore seu desempenho ao longo das épocas.\n",
    "3. Qual a diferença fundamental entre os modos model.train() e model.eval()?\n",
    "> model.train() ativa o modo de treino, onde Dropout e BatchNorm funcionam com comportamento de treinamento. Já model.eval() é usado na avaliação, desativando Dropout e usando estatísticas fixas no BatchNorm, garantindo resultados estáveis e reprodutíveis.\n",
    "\n",
    ">Treino: processamento + cálculo de gradientes + atualização dos pesos.\n",
    "Avaliação: processamento somente (sem atualização), com camadas adaptadas para estabilidade.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66de4b93",
   "metadata": {},
   "source": [
    "\n",
    "# 3 - Modifique a classe ClassBin para que a rede tenha a seguinte arquitetura:\n",
    "\n",
    "1. Camada de Entrada: Mantém as 4 features de entrada.\n",
    "2. Primeira Camada Oculta: nn.Linear com 4 neurônios de entrada e 16 neurônios de saída, seguida por uma ativação ReLU.\n",
    "3. Segunda Camada Oculta: nn.Linear com 16 neurônios de entrada e 8 neurônios de saída, seguida por uma ativação ReLU.\n",
    "4. Camada de Saída: nn.Linear com 8 neurônios de entrada e 1 neurônio de saída, seguida por uma ativação Sigmoid.\n",
    "5. Remova todas as camadas de Dropout para uma nova arquitetura.\n",
    "6. Treine o novo modelo com os mesmos hiperparâmetros (épocas, taxa de aprendizado, etc.) e compare a acurácia final (de treino e teste) com a do modelo original.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d74c247d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ClassBin(nn.Module):\n",
    "    # Construtor\n",
    "    def __init__(self):\n",
    "        super(ClassBin, self).__init__()\n",
    "        self.linear1 = nn.Linear(4, 16)    # primeira hidden layer\n",
    "        self.linear2 = nn.Linear(16, 8)    # segunda hidden layer\n",
    "        self.linear3 = nn.Linear(8, 1)    # terceira hidden layer --> Camada de saída\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "\n",
    "    # Propagação (Feed Forward)\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.linear1(x))\n",
    "        x = F.relu(self.linear2(x))\n",
    "        x = F.relu(self.linear3(x))\n",
    "        x = self.sigmoid(x)\n",
    "        return x\n",
    "\n",
    "model = ClassBin()\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07bfe45a",
   "metadata": {},
   "source": [
    "## Com os mesmos hiperparâmetros:\n",
    "---------------------------\n",
    "Modelo Original:\n",
    "\n",
    "Acurácia de Treino: 0.590654194355011\n",
    "\n",
    "Acurácia de Teste: 0.6033519506454468\n",
    "\n",
    "---------------------------\n",
    "Modelo modificado:\n",
    "\n",
    "Acurácia de Treino: 0.590654194355011\n",
    "\n",
    "Acurácia de Teste: 0.6033519506454468\n",
    "\n",
    "---------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35824f31",
   "metadata": {},
   "source": [
    "\n",
    "# 4 -  Usando o modelo original do notebook:\n",
    "\n",
    "1. Mude o Otimizador: Substitua o otimizador **Adam** por SGD (Stochastic Gradient Descent). \n",
    "2. Treine o modelo com o SGD.\n",
    "3. O que aconteceu com o custo (loss) durante o treinamento? A acurácia final foi melhor ou pior? O SGD com essa taxa de aprendizado pareceu uma boa escolha?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f33d6a4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_fn = nn.BCELoss()\n",
    "epochs = 100\n",
    "batch_size = 32  # X_train 535 / 32 = 16.71 (então são 17 batches de 32)\n",
    "learning_rate = 0.1\n",
    "\n",
    "# Instânciar o Otimizador SGD\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0235f7d0",
   "metadata": {},
   "source": [
    "Durante o treinamento, o custo se tornou um pouco variável nas épocas (Com Adam era sempre 0.693). Veja as 10 primeiras épocas:\n",
    "\n",
    "Época 1,  Custo Treino: 0.685\n",
    "\n",
    "Época 2,  Custo Treino: 0.693\n",
    "\n",
    "Época 3,  Custo Treino: 0.667\n",
    "\n",
    "Época 4,  Custo Treino: 0.693\n",
    "\n",
    "Época 5,  Custo Treino: 0.693\n",
    "\n",
    "Época 6,  Custo Treino: 0.693\n",
    "\n",
    "Época 7,  Custo Treino: 0.693\n",
    "\n",
    "Época 8,  Custo Treino: 0.693\n",
    "\n",
    "Época 9,  Custo Treino: 0.815\n",
    "\n",
    "Época 10, Custo Treino: 0.693\n",
    "\n",
    "-----\n",
    "Adam:\n",
    "\n",
    "Acurácia de Treino: 0.590654194355011\n",
    "\n",
    "Acurácia de Teste: 0.6033519506454468\n",
    "\n",
    "SGD:\n",
    "\n",
    "Acurácia de Treino: 0.590654194355011\n",
    "\n",
    "Acurácia de Teste: 0.6033519506454468\n",
    "\n",
    "------\n",
    "\n",
    "Com learning_rate = 0.1, não foi possível identificar diferenças signicativas entre os dois otimizadores.\n",
    "\n",
    "Com learning_rate = 0.001:\n",
    "\n",
    "Adam:\n",
    "\n",
    "Acurácia de Treino: 0.590654194355011\n",
    "\n",
    "Acurácia de Teste: 0.6033519506454468\n",
    "\n",
    "SGD: \n",
    "\n",
    "Acurácia de Treino: 0.6523364782333374\n",
    "\n",
    "Acurácia de Teste: 0.6424580812454224\n",
    "\n",
    "Com o novo learnig_rate, o SGD se provou uma boa escolha.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c177ed8",
   "metadata": {},
   "source": [
    "\n",
    "# 5 - Usando o modelo original e o otimizador Adam:\n",
    "\n",
    "1. No DataLoader, mude o batch_size de 32 para um valor muito maior, como 512.\n",
    "2. Treine o modelo e observe a acurácia.\n",
    "3. Agora, faça o oposto. Mude o batch_size para um valor bem pequeno, como 4, e treine novamente.\n",
    "4. Como a mudança no batch_size afetou a estabilidade do custo (loss) a cada época e a acurácia final do modelo?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00b56c9e",
   "metadata": {},
   "source": [
    "#### batch_size = 512\n",
    "\n",
    ">O custo(loss) de cada época se estabilizou na época 12\n",
    "\n",
    ">Rodou bem rápido --> X_train 535 / 512 = 1.05 (então são 2 batches de 512)\n",
    "\n",
    ">Acurácia de Treino: 0.590654194355011\n",
    "\n",
    ">Acurácia de Teste: 0.6033519506454468\n",
    "\n",
    "#### batch_size = 4\n",
    "\n",
    ">O custo(loss) de cada época foi sempre o mesmo(estabilizou rápido).\n",
    "\n",
    ">Demorou mais para rodar --> X_train 535 / 4 = 133.75 (então são 134 batches de 4)\n",
    "\n",
    ">Acurácia de Treino: 0.590654194355011\n",
    "\n",
    ">Acurácia de Teste: 0.6033519506454468\n",
    "\n",
    "#### Mudei a learning_rate para 0.001\n",
    "\n",
    "#### batch_size = 512:\n",
    ">Acurácia de Treino: 0.6504672765731812\n",
    "\n",
    ">Acurácia de Teste: 0.6480447053909302\n",
    "\n",
    "#### batch_size = 4\n",
    ">Acurácia de Treino: 0.7831775546073914\n",
    "\n",
    ">Acurácia de Teste: 0.7262569665908813\n",
    "\n",
    "#### Conclusão\n",
    "\n",
    ">Um batch_size menor pode favorecer a acurácia do modelo, mas exige mais tempo de treinamento, o que precisa ser considerado.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
